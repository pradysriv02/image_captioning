{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#location of the data \ndata_location =  \"/kaggle/input/flickr8k\"\n!ls $data_location","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#reading the text data \nimport pandas as pd\ncaption_file = data_location + '/captions.txt'\ndf = pd.read_csv(caption_file)\nprint(\"There are {} image to captions\".format(len(df)))\ndf.head(7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n#select any index from the whole dataset \n#single image has 5 captions\n#so, select indx as: 0,5,10.....\ndata_idx = 0\n\n\n#eg path to be plot: ../input/flickr8k/Images/1000268201_693b08cb0e.jpg\nimage_path = data_location+\"/Images/\"+df.iloc[data_idx,0]\nimg=mpimg.imread(image_path)\nplt.imshow(img)\nplt.show()\n\n#image consits of 5 captions,\n#showing all 5 captions of the image of the given idx \nfor i in range(data_idx,data_idx+5):\n    print(\"Caption:\",df.iloc[i,1])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#imports \nimport os\nfrom collections import Counter\nimport spacy\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader,Dataset\nimport torchvision.transforms as T\n\nfrom PIL import Image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install spacy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#using spacy for the better text tokenization \nspacy_eng = spacy.blank(\"en\")\n\n#example\ntext = \"This is a good place to find a city\"\n[token.text.lower() for token in spacy_eng.tokenizer(text)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Vocabulary:\n    def __init__(self,freq_threshold):\n        #setting the pre-reserved tokens int to string tokens\n        self.itos = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n        \n        #string to int tokens\n        #its reverse dict self.itos\n        self.stoi = {v:k for k,v in self.itos.items()}\n        \n        self.freq_threshold = freq_threshold\n        \n    def __len__(self): return len(self.itos)\n    \n    @staticmethod\n    def tokenize(text):\n        return [token.text.lower() for token in spacy_eng.tokenizer(text)]\n    \n    def build_vocab(self, sentence_list):\n        frequencies = Counter()\n        idx = 4\n        \n        for sentence in sentence_list:\n            for word in self.tokenize(sentence):\n                frequencies[word] += 1\n                \n                #add the word to the vocab if it reaches minum frequecy threshold\n                if frequencies[word] == self.freq_threshold:\n                    self.stoi[word] = idx\n                    self.itos[idx] = word\n                    idx += 1\n    def numericalize(self,text):\n        \"\"\" For each word in the text corresponding index token for that word form the vocab built as list \"\"\"\n        tokenized_text = self.tokenize(text)\n        return [ self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text ]    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#testing the vicab class \nv = Vocabulary(freq_threshold=1)\n\nv.build_vocab([\"i know my name\"])\nprint(v.stoi)\nprint(v.numericalize(\"i know my name>\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FlickrDataset(Dataset):\n    \"\"\"\n    FlickrDataset\n    \"\"\"\n    def __init__(self,root_dir,caption_file,transform=None,freq_threshold=5):\n        self.root_dir = root_dir\n        self.df = pd.read_csv(caption_file)\n        self.transform = transform\n        \n        #Get image and caption colum from the dataframe\n        self.imgs = self.df[\"image\"]\n        self.captions = self.df[\"caption\"]\n        \n        #Initialize vocabulary and build vocab\n        self.vocab = Vocabulary(freq_threshold)\n        self.vocab.build_vocab(self.captions.tolist())\n        \n    \n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self,idx):\n        caption = self.captions[idx]\n        img_name = self.imgs[idx]\n        img_location = os.path.join(self.root_dir,img_name)\n        img = Image.open(img_location).convert(\"RGB\")\n        \n        #apply the transfromation to the image\n        if self.transform is not None:\n            img = self.transform(img)\n        \n        #numericalize the caption text\n        caption_vec = []\n        caption_vec += [self.vocab.stoi[\"<SOS>\"]]\n        caption_vec += self.vocab.numericalize(caption)\n        caption_vec += [self.vocab.stoi[\"<EOS>\"]]\n        \n        return img, torch.tensor(caption_vec)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#defing the transform to be applied\ntransforms = T.Compose([\n    T.Resize((224,224)),\n    T.ToTensor()\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_image(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#testing the dataset class\ndataset =  FlickrDataset(\n    root_dir = data_location+\"/Images\",\n    caption_file = data_location+\"/captions.txt\",\n    transform=transforms\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img, caps = dataset[56]\nshow_image(img,\"Image\")\nprint(\"Token:\",caps)\nprint(\"Sentence:\")\nprint([dataset.vocab.itos[token] for token in caps.tolist()])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CapsCollate:\n    \"\"\"\n    Collate to apply the padding to the captions with dataloader\n    \"\"\"\n    def __init__(self,pad_idx,batch_first=False):\n        self.pad_idx = pad_idx\n        self.batch_first = batch_first\n    \n    def __call__(self,batch):\n        imgs = [item[0].unsqueeze(0) for item in batch]\n        imgs = torch.cat(imgs,dim=0)\n        \n        targets = [item[1] for item in batch]\n        targets = pad_sequence(targets, batch_first=self.batch_first, padding_value=self.pad_idx)\n        return imgs,targets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#writing the dataloader\n#setting the constants\nBATCH_SIZE = 4\nNUM_WORKER = 1\n\n#token to represent the padding\npad_idx = dataset.vocab.stoi[\"<PAD>\"]\n\ndata_loader = DataLoader(\n    dataset=dataset,\n    batch_size=BATCH_SIZE,\n    num_workers=NUM_WORKER,\n    shuffle=True,\n    collate_fn=CapsCollate(pad_idx=pad_idx,batch_first=True)\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#generating the iterator from the dataloader\ndataiter = iter(data_loader)\n\n#getting the next batch\nbatch = next(dataiter)\n\n#unpacking the batch\nimages, captions = batch\n\n#showing info of image in single batch\nfor i in range(BATCH_SIZE):\n    img,cap = images[i],captions[i]\n    caption_label = [dataset.vocab.itos[token] for token in cap.tolist()]\n    eos_index = caption_label.index('<EOS>')\n    caption_label = caption_label[1:eos_index]\n    caption_label = ' '.join(caption_label)                      \n    show_image(img,caption_label)\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #location of the training data \n# data_location =  \"../input/flickr8k\"\n# #copy dataloader\n# !cp ../input/data-loader/data_loader.py .\n\n#imports\nimport numpy as np\nimport torch\n# from torch.utils.data import DataLoader,Dataset\nimport torchvision.transforms as T\n\n#custom imports \n# from data_loader import FlickrDataset,get_data_loader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#show the tensor image\nimport matplotlib.pyplot as plt\ndef show_image(img, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    \n    #unnormalize \n    img[0] = img[0] * 0.229\n    img[1] = img[1] * 0.224 \n    img[2] = img[2] * 0.225 \n    img[0] += 0.485 \n    img[1] += 0.456 \n    img[2] += 0.406\n    \n    img = img.numpy().transpose((1, 2, 0))\n    \n    \n    plt.imshow(img)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Initiate the Dataset and Dataloader\n\n#setting the constants\ndata_location =  \"/kaggle/input/flickr8k\"\n# BATCH_SIZE = 256\nBATCH_SIZE = 6\nNUM_WORKER = 4\n\n#defining the transform to be applied\ntransforms = T.Compose([\n    T.Resize(226),                     \n    T.RandomCrop(224),                 \n    T.ToTensor(),                               \n    T.Normalize((0.485, 0.456, 0.406),(0.229, 0.224, 0.225))\n])\n\n\n#testing the dataset class\ndataset =  FlickrDataset(\n    root_dir = data_location+\"/Images\",\n    caption_file = data_location+\"/captions.txt\",\n    transform=transforms\n)\n\n# #writing the dataloader\n# data_loader = get_data_loader(\n#     dataset=dataset,\n#     batch_size=BATCH_SIZE,\n#     num_workers=NUM_WORKER,\n#     shuffle=True,\n#     # batch_first=False\n# )\ndata_loader = DataLoader(\n    dataset=dataset,\n    batch_size=BATCH_SIZE,\n    num_workers=NUM_WORKER,\n    shuffle=True,\n    collate_fn=CapsCollate(pad_idx=pad_idx,batch_first=True)\n)\n\n#vocab_size\nvocab_size = len(dataset.vocab)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Defining the Model ArchitectureÂ¶\nModel is seq2seq model. In the encoder pretrained ResNet model is used to extract the features. Decoder, is the implementation of the Bahdanau Attention Decoder. In the decoder model LSTM cell.","metadata":{}},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport torchvision.models as models\nfrom torch.utils.data import DataLoader,Dataset\nimport torchvision.transforms as T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EncoderCNN(nn.Module):\n    def __init__(self):\n        super(EncoderCNN, self).__init__()\n        resnet = models.resnet50(pretrained=True)\n        for param in resnet.parameters():\n            param.requires_grad_(False)\n        \n        modules = list(resnet.children())[:-2]\n        self.resnet = nn.Sequential(*modules)\n        \n\n    def forward(self, images):\n        features = self.resnet(images)                                    #(batch_size,2048,7,7)\n        features = features.permute(0, 2, 3, 1)                           #(batch_size,7,7,2048)\n        features = features.view(features.size(0), -1, features.size(-1)) #(batch_size,49,2048)\n        return features\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Bahdanau Attention\nclass Attention(nn.Module):\n    def __init__(self, encoder_dim,decoder_dim,attention_dim):\n        super(Attention, self).__init__()\n        \n        self.attention_dim = attention_dim\n        \n        self.W = nn.Linear(decoder_dim,attention_dim)\n        self.U = nn.Linear(encoder_dim,attention_dim)\n        \n        self.A = nn.Linear(attention_dim,1)\n        \n        \n        \n        \n    def forward(self, features, hidden_state):\n        u_hs = self.U(features)     #(batch_size,num_layers,attention_dim)\n        w_ah = self.W(hidden_state) #(batch_size,attention_dim)\n        \n        combined_states = torch.tanh(u_hs + w_ah.unsqueeze(1)) #(batch_size,num_layers,attemtion_dim)\n        \n        attention_scores = self.A(combined_states)         #(batch_size,num_layers,1)\n        attention_scores = attention_scores.squeeze(2)     #(batch_size,num_layers)\n        \n        \n        alpha = F.softmax(attention_scores,dim=1)          #(batch_size,num_layers)\n        \n        attention_weights = features * alpha.unsqueeze(2)  #(batch_size,num_layers,features_dim)\n        attention_weights = attention_weights.sum(dim=1)   #(batch_size,num_layers)\n        \n        return alpha,attention_weights\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Attention Decoder\nclass DecoderRNN(nn.Module):\n    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n        super().__init__()\n        \n        #save the model param\n        self.vocab_size = vocab_size\n        self.attention_dim = attention_dim\n        self.decoder_dim = decoder_dim\n        \n        self.embedding = nn.Embedding(vocab_size,embed_size)\n        self.attention = Attention(encoder_dim,decoder_dim,attention_dim)\n        \n        \n        self.init_h = nn.Linear(encoder_dim, decoder_dim)  \n        self.init_c = nn.Linear(encoder_dim, decoder_dim)  \n        self.lstm_cell = nn.LSTMCell(embed_size+encoder_dim,decoder_dim,bias=True)\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n        \n        \n        self.fcn = nn.Linear(decoder_dim,vocab_size)\n        self.drop = nn.Dropout(drop_prob)\n        \n        \n    \n    def forward(self, features, captions):\n        \n        #vectorize the caption\n        embeds = self.embedding(captions)\n        \n        # Initialize LSTM state\n        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n        \n        #get the seq length to iterate\n        seq_length = len(captions[0])-1 #Exclude the last one\n        batch_size = captions.size(0)\n        num_features = features.size(1)\n        \n        preds = torch.zeros(batch_size, seq_length, self.vocab_size).to(device)\n        alphas = torch.zeros(batch_size, seq_length,num_features).to(device)\n                \n        for s in range(seq_length):\n            alpha,context = self.attention(features, h)\n            lstm_input = torch.cat((embeds[:, s], context), dim=1)\n            h, c = self.lstm_cell(lstm_input, (h, c))\n                    \n            output = self.fcn(self.drop(h))\n            \n            preds[:,s] = output\n            alphas[:,s] = alpha  \n        \n        \n        return preds, alphas\n    \n    def generate_caption(self,features,max_len=20,vocab=None):\n        # Inference part\n        # Given the image features generate the captions\n        \n        batch_size = features.size(0)\n        h, c = self.init_hidden_state(features)  # (batch_size, decoder_dim)\n        \n        alphas = []\n        \n        #starting input\n        word = torch.tensor(vocab.stoi['<SOS>']).view(1,-1).to(device)\n        embeds = self.embedding(word)\n\n        \n        captions = []\n        \n        for i in range(max_len):\n            alpha,context = self.attention(features, h)\n            \n            \n            #store the apla score\n            alphas.append(alpha.cpu().detach().numpy())\n            \n            lstm_input = torch.cat((embeds[:, 0], context), dim=1)\n            h, c = self.lstm_cell(lstm_input, (h, c))\n            output = self.fcn(self.drop(h))\n            output = output.view(batch_size,-1)\n        \n            \n            #select the word with most val\n            predicted_word_idx = output.argmax(dim=1)\n            \n            #save the generated word\n            captions.append(predicted_word_idx.item())\n            \n            #end if <EOS detected>\n            if vocab.itos[predicted_word_idx.item()] == \"<EOS>\":\n                break\n            \n            #send generated word as the next caption\n            embeds = self.embedding(predicted_word_idx.unsqueeze(0))\n        \n        #covert the vocab idx to words and return sentence\n        return [vocab.itos[idx] for idx in captions],alphas\n    \n    \n    def init_hidden_state(self, encoder_out):\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n        c = self.init_c(mean_encoder_out)\n        return h, c\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EncoderDecoder(nn.Module):\n    def __init__(self,embed_size, vocab_size, attention_dim,encoder_dim,decoder_dim,drop_prob=0.3):\n        super().__init__()\n        self.encoder = EncoderCNN()\n        self.decoder = DecoderRNN(\n            embed_size=embed_size,\n#             vocab_size = len(dataset.vocab),\n            vocab_size=vocab_size,\n            attention_dim=attention_dim,\n            encoder_dim=encoder_dim,\n            decoder_dim=decoder_dim\n        )\n        \n    def forward(self, images, captions):\n        features = self.encoder(images)\n        outputs = self.decoder(features, captions)\n        return outputs\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Hyperparams\nembed_size=300\nvocab_size = len(dataset.vocab)\nattention_dim=256\nencoder_dim=2048\ndecoder_dim=512\nlearning_rate = 3e-4\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#init model\nmodel = EncoderDecoder(\n    embed_size=300,\n    vocab_size = len(dataset.vocab),\n    attention_dim=256,\n    encoder_dim=2048,\n    decoder_dim=512\n).to(device)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#helper function to save the model\ndef save_model(model,num_epochs):\n    model_state = {\n        'num_epochs':num_epochs,\n        'embed_size':embed_size,\n        'vocab_size':len(dataset.vocab),\n        'attention_dim':attention_dim,\n        'encoder_dim':encoder_dim,\n        'decoder_dim':decoder_dim,\n        'state_dict':model.state_dict()\n    }\n    torch.save(model_state, '/kaggle/working/attention_model_state.pth')\n    \n\n#     torch.save(model_state,'attention_model_state.pth')\n    # Save the trained model\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nnum_epochs = 15\nprint_every = 100\n\nfor epoch in range(1,num_epochs+1):   \n    for idx, (image, captions) in enumerate(iter(data_loader)):\n        image,captions = image.to(device),captions.to(device)\n\n        # Zero the gradients.\n        optimizer.zero_grad()\n\n        # Feed forward\n        outputs,attentions = model(image, captions)\n\n        # Calculate the batch loss.\n        targets = captions[:,1:]\n        loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n        \n        # Backward pass.\n        loss.backward()\n\n        # Update the parameters in the optimizer.\n        optimizer.step()\n\n        if (idx+1)%print_every == 0:\n            print(\"Epoch: {} loss: {:.5f}\".format(epoch,loss.item()))\n            \n            \n            #generate the caption\n            model.eval()\n            with torch.no_grad():\n                dataiter = iter(data_loader)\n                img,_ = next(dataiter)\n                features = model.encoder(img[0:1].to(device))\n                caps,alphas = model.decoder.generate_caption(features,vocab=dataset.vocab)\n                caption = ' '.join(caps)\n                show_image(img[0],title=caption)\n                \n            model.train()\n        \n    #save the latest model\n    save_model(model,epoch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the saved model\nsaved_model_path = \"/kaggle/working/attention_model_state.pth\"\n# saved_model_state = torch.load(saved_model_path)\ncheckpoint = torch.load(saved_model_path)\nmodel.load_state_dict(checkpoint['state_dict'])\n# saved_model_state = {\n#     'embed_size': 300,\n#     'vocab_size': 10000,\n#     'attention_dim': 512,\n#     'encoder_dim': 512,\n#     'decoder_dim': 512\n# }\n# model = EncoderDecoder(\n#     embed_size=saved_model_state['embed_size'],\n#     vocab_size=saved_model_state['vocab_size'],\n#     attention_dim=saved_model_state['attention_dim'],\n#     encoder_dim=saved_model_state['encoder_dim'],\n#     decoder_dim=saved_model_state['decoder_dim']\n# ).to(device)\n# model.load_state_dict(saved_model_state)\n\n# # Set the model to evaluation mode\n# model.eval()\n\n# # Define the image transform for testing\n# test_transforms = T.Compose([\n#     T.Resize(226),\n#     T.CenterCrop(224),\n#     T.ToTensor(),\n#     T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n# ])\n\n# # Load and preprocess the test image\n# test_image_path = \"/kaggle/input/image2/a4e1c55a9.png\"  # Replace with the actual path to the test image\n# test_image = Image.open(test_image_path).convert(\"RGB\")\n# test_image = test_transforms(test_image).unsqueeze(0).to(device)\n\n# # Generate the caption for the test image\n# generated_caption, _ = model.generate_caption(test_image, max_len=20, vocab=dataset.vocab)\n\n# # Print the generated caption\n# print(\"Generated Caption: \", \" \".join(generated_caption))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# generate the captions for a whole batch\nmodel.eval()\nwith torch.no_grad():\n    # dls or validation_loader\n    itr = iter(data_loader)\n    img, captions= next(itr)\n    \n    caption = captions[0:1][0].tolist()\n    s = [dataset.vocab.itos[idx] for idx in caption if idx != 0] # if idx != 0 and idx != 1 and idx != 2\n    print(\"Original:\", ' '.join(s))\n    \n    # extract features\n    print(img[0:1].shape)\n    features = model.encoder(img[0:1].to(device))\n\n    # get predictions\n    pred_caps, alphas = model.decoder.generate_caption(features, vocab=dataset.vocab)\n\n    # make it printable\n    caption = ' '.join(pred_caps)\n    print(\"Predicted:\", caption)\n    show_image(img[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import transforms\nimport matplotlib.pyplot as plt\nmodel.eval()\nwith torch.no_grad():\n    # Load a single image\n   \n\n    # Load and preprocess a single image\n    image_path = \"/kaggle/input/flickr8k/Images/1000268201_693b08cb0e.jpg\"  # Replace with the actual path to your image\n    image = Image.open(image_path).convert(\"RGB\")\n\n    preprocess = transforms.Compose([\n        transforms.Resize(226),\n        transforms.RandomCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n    ])\n\n    processed_image = preprocess(image)\n\n    # Add a batch dimension to the processed image\n    processed_image = processed_image.unsqueeze(0)\n\n    \n    features = model.encoder(processed_image.to(device))\n    pred_caps, alphas = model.decoder.generate_caption(features, vocab=dataset.vocab)\n    predicted_caption = ' '.join(pred_caps)\n# Convert the processed image tensor to a numpy array\nimage = processed_image.squeeze(0).permute(1, 2, 0).cpu().numpy()\n\n# Display the image\nplt.imshow(image)\nplt.axis('off')\nplt.show()\nprint(\"Predicted Caption:\", predicted_caption)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu\n\n# Assuming 'references' is a list of reference captions for the image\n# and 'predicted_caption' is the predicted caption generated by your model\nreferences = [['A child in a pink dress is climbing up a set of stairs in an entry way .', 'A girl going into a wooden building .', 'A little girl climbing into a wooden playhouse .','A little girl climbing the stairs to her playhouse .','A little girl in a pink dress going into a wooden cabin .']]\npredicted_caption = ' '.join(pred_caps)\n\n# Convert the predicted caption into a list of tokens\npredicted_tokens = predicted_caption.split()\n\n# Convert the reference captions into a list of lists of tokens\nreference_tokens = [ref.split() for ref in references[0]]\n\n# Calculate the BLEU score\nbleu_score = corpus_bleu([reference_tokens], [predicted_tokens])\n\n# Print the BLEU score\nprint(\"BLEU Score:\", bleu_score)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}